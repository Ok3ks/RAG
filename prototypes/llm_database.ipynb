{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.llms import OpenAI,Cohere, Anthropic\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, FewShotPromptTemplate\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "#from langchain.document_loaders import DataFrameLoader\n",
    "\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from langchain.chains import LLMChain\n",
    "from typing import List\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test out with querying csvs and pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='state: state\\nviolent: violent\\nmurder: murder\\nnhs_grad: hs_grad\\npoverty: poverty\\nsingle: single\\nwhite: white\\nurban: urban', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 0}, lookup_index=0), Document(page_content='state: Alabama\\nviolent: 459.9\\nmurder: 7.1\\nnhs_grad: 82.1\\npoverty: 17.5\\nsingle: 29.0\\nwhite: 70\\nurban: 48.65', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 1}, lookup_index=0), Document(page_content='state: Alaska\\nviolent: 632.6\\nmurder: 3.2\\nnhs_grad: 91.4\\npoverty: 9.0\\nsingle: 25.5\\nwhite: 68.3\\nurban: 44.46', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 2}, lookup_index=0), Document(page_content='state: Arizona\\nviolent: 423.2\\nmurder: 5.5\\nnhs_grad: 84.2\\npoverty: 16.5\\nsingle: 25.7\\nwhite: 80\\nurban: 80.07', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 3}, lookup_index=0), Document(page_content='state: Arkansas\\nviolent: 530.3\\nmurder: 6.3\\nnhs_grad: 82.4\\npoverty: 18.8\\nsingle: 26.3\\nwhite: 78.4\\nurban: 39.54', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 4}, lookup_index=0), Document(page_content='state: California\\nviolent: 473.4\\nmurder: 5.4\\nnhs_grad: 80.6\\npoverty: 14.2\\nsingle: 27.8\\nwhite: 62.7\\nurban: 89.73', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 5}, lookup_index=0), Document(page_content='state: Colorado\\nviolent: 340.9\\nmurder: 3.2\\nnhs_grad: 89.3\\npoverty: 12.9\\nsingle: 21.4\\nwhite: 84.6\\nurban: 76.86', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 6}, lookup_index=0), Document(page_content='state: Connecticut\\nviolent: 300.5\\nmurder: 3.0\\nnhs_grad: 88.6\\npoverty: 9.4\\nsingle: 25.0\\nwhite: 79.1\\nurban: 84.83', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 7}, lookup_index=0), Document(page_content='state: Delaware\\nviolent: 645.1\\nmurder: 4.6\\nnhs_grad: 87.4\\npoverty: 10.8\\nsingle: 27.6\\nwhite: 71.9\\nurban: 68.71', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 8}, lookup_index=0), Document(page_content='state: District of Columbia\\nviolent: 1348.9\\nmurder: 24.2\\nnhs_grad: 87.1\\npoverty: 18.4\\nsingle: 48.0\\nwhite: 38.7\\nurban: 100', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 9}, lookup_index=0), Document(page_content='state: Florida\\nviolent: 612.6\\nmurder: 5.5\\nnhs_grad: 85.3\\npoverty: 14.9\\nsingle: 26.6\\nwhite: 76.9\\nurban: 87.44', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 10}, lookup_index=0), Document(page_content='state: Georgia\\nviolent: 432.6\\nmurder: 6.0\\nnhs_grad: 83.9\\npoverty: 16.5\\nsingle: 29.3\\nwhite: 61.9\\nurban: 65.38', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 11}, lookup_index=0), Document(page_content='state: Hawaii\\nviolent: 274.1\\nmurder: 1.8\\nnhs_grad: 90.4\\npoverty: 10.4\\nsingle: 26.3\\nwhite: 26.9\\nurban: 71.46', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 12}, lookup_index=0), Document(page_content='state: Idaho\\nviolent: 238.5\\nmurder: 1.5\\nnhs_grad: 88.4\\npoverty: 14.3\\nsingle: 19.0\\nwhite: 92.3\\nurban: 50.51', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 13}, lookup_index=0), Document(page_content='state: Illinois\\nviolent: 618.2\\nmurder: 8.4\\nnhs_grad: 86.4\\npoverty: 13.3\\nsingle: 26.0\\nwhite: 72.5\\nurban: 79.97', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 14}, lookup_index=0), Document(page_content='state: Indiana\\nviolent: 366.4\\nmurder: 5.3\\nnhs_grad: 86.6\\npoverty: 14.4\\nsingle: 24.5\\nwhite: 85.7\\nurban: 59.17', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 15}, lookup_index=0), Document(page_content='state: Iowa\\nviolent: 294.5\\nmurder: 1.3\\nnhs_grad: 90.5\\npoverty: 11.8\\nsingle: 20.3\\nwhite: 92.3\\nurban: 41.66', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 16}, lookup_index=0), Document(page_content='state: Kansas\\nviolent: 412.0\\nmurder: 4.7\\nnhs_grad: 89.7\\npoverty: 13.4\\nsingle: 22.8\\nwhite: 86.3\\nurban: 50.17', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 17}, lookup_index=0), Document(page_content='state: Kentucky\\nviolent: 265.5\\nmurder: 4.3\\nnhs_grad: 81.7\\npoverty: 18.6\\nsingle: 25.4\\nwhite: 88.8\\nurban: 40.99', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 18}, lookup_index=0), Document(page_content='state: Louisiana\\nviolent: 628.4\\nmurder: 12.3\\nnhs_grad: 82.2\\npoverty: 17.3\\nsingle: 31.4\\nwhite: 63.7\\nurban: 61.33', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 19}, lookup_index=0), Document(page_content='state: Maine\\nviolent: 119.9\\nmurder: 2.0\\nnhs_grad: 90.2\\npoverty: 12.3\\nsingle: 22.0\\nwhite: 94.9\\nurban: 26.21', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 20}, lookup_index=0), Document(page_content='state: Maryland\\nviolent: 590.0\\nmurder: 7.7\\nnhs_grad: 88.2\\npoverty: 9.1\\nsingle: 27.3\\nwhite: 60.2\\nurban: 83.53', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 21}, lookup_index=0), Document(page_content='state: Massachusetts\\nviolent: 465.6\\nmurder: 2.7\\nnhs_grad: 89.0\\npoverty: 10.3\\nsingle: 25.0\\nwhite: 82.4\\nurban: 90.3', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 22}, lookup_index=0), Document(page_content='state: Michigan\\nviolent: 504.4\\nmurder: 6.3\\nnhs_grad: 87.9\\npoverty: 16.2\\nsingle: 25.6\\nwhite: 79.9\\nurban: 66.37', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 23}, lookup_index=0), Document(page_content='state: Minnesota\\nviolent: 214.2\\nmurder: 1.5\\nnhs_grad: 91.5\\npoverty: 11.0\\nsingle: 20.2\\nwhite: 87.4\\nurban: 58', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 24}, lookup_index=0), Document(page_content='state: Mississippi\\nviolent: 306.7\\nmurder: 6.9\\nnhs_grad: 80.4\\npoverty: 21.9\\nsingle: 32.8\\nwhite: 59.6\\nurban: 27.62', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 25}, lookup_index=0), Document(page_content='state: Missouri\\nviolent: 500.3\\nmurder: 6.6\\nnhs_grad: 86.8\\npoverty: 14.6\\nsingle: 25.3\\nwhite: 83.9\\nurban: 56.61', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 26}, lookup_index=0), Document(page_content='state: Montana\\nviolent: 283.9\\nmurder: 3.2\\nnhs_grad: 90.8\\npoverty: 15.1\\nsingle: 20.3\\nwhite: 89.4\\nurban: 26.49', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 27}, lookup_index=0), Document(page_content='state: Nebraska\\nviolent: 305.5\\nmurder: 2.5\\nnhs_grad: 89.8\\npoverty: 12.3\\nsingle: 20.9\\nwhite: 88.1\\nurban: 53.78', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 28}, lookup_index=0), Document(page_content='state: Nevada\\nviolent: 704.6\\nmurder: 5.9\\nnhs_grad: 83.9\\npoverty: 12.4\\nsingle: 28.5\\nwhite: 76.2\\nurban: 86.51', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 29}, lookup_index=0), Document(page_content='state: New Hampshire\\nviolent: 169.5\\nmurder: 0.9\\nnhs_grad: 91.3\\npoverty: 8.5\\nsingle: 19.5\\nwhite: 94.5\\nurban: 47.34', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 30}, lookup_index=0), Document(page_content='state: New Jersey\\nviolent: 311.3\\nmurder: 3.7\\nnhs_grad: 87.4\\npoverty: 9.4\\nsingle: 25.8\\nwhite: 70.7\\nurban: 92.24', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 31}, lookup_index=0), Document(page_content='state: New Mexico\\nviolent: 652.8\\nmurder: 10.0\\nnhs_grad: 82.8\\npoverty: 18.0\\nsingle: 29.1\\nwhite: 72.5\\nurban: 53.75', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 32}, lookup_index=0), Document(page_content='state: New York\\nviolent: 385.5\\nmurder: 4.0\\nnhs_grad: 84.7\\npoverty: 14.2\\nsingle: 30.2\\nwhite: 67.4\\nurban: 82.66', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 33}, lookup_index=0), Document(page_content='state: North Carolina\\nviolent: 414.0\\nmurder: 5.4\\nnhs_grad: 84.3\\npoverty: 16.3\\nsingle: 26.3\\nwhite: 70.5\\nurban: 54.88', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 34}, lookup_index=0), Document(page_content='state: North Dakota\\nviolent: 223.6\\nmurder: 2.0\\nnhs_grad: 90.1\\npoverty: 11.7\\nsingle: 18.2\\nwhite: 90.2\\nurban: 40', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 35}, lookup_index=0), Document(page_content='state: Ohio\\nviolent: 358.1\\nmurder: 5.0\\nnhs_grad: 87.6\\npoverty: 15.2\\nsingle: 26.3\\nwhite: 84\\nurban: 65.31', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 36}, lookup_index=0), Document(page_content='state: Oklahoma\\nviolent: 510.4\\nmurder: 6.5\\nnhs_grad: 85.6\\npoverty: 16.2\\nsingle: 25.9\\nwhite: 75.4\\nurban: 45.79', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 37}, lookup_index=0), Document(page_content='state: Oregon\\nviolent: 261.2\\nmurder: 2.3\\nnhs_grad: 89.1\\npoverty: 14.3\\nsingle: 22.7\\nwhite: 85.6\\nurban: 62.47', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 38}, lookup_index=0), Document(page_content='state: Pennsylvania\\nviolent: 388.9\\nmurder: 5.4\\nnhs_grad: 87.9\\npoverty: 12.5\\nsingle: 24.5\\nwhite: 83.5\\nurban: 70.68', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 39}, lookup_index=0), Document(page_content='state: Rhode Island\\nviolent: 254.3\\nmurder: 3.0\\nnhs_grad: 84.7\\npoverty: 11.5\\nsingle: 27.3\\nwhite: 82.6\\nurban: 90.46', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 40}, lookup_index=0), Document(page_content='state: South Carolina\\nviolent: 675.1\\nmurder: 6.7\\nnhs_grad: 83.6\\npoverty: 17.1\\nsingle: 28.4\\nwhite: 67.6\\nurban: 55.78', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 41}, lookup_index=0), Document(page_content='state: South Dakota\\nviolent: 201.0\\nmurder: 3.6\\nnhs_grad: 89.9\\npoverty: 14.2\\nsingle: 20.8\\nwhite: 86.3\\nurban: 29.92', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 42}, lookup_index=0), Document(page_content='state: Tennessee\\nviolent: 666.0\\nmurder: 7.4\\nnhs_grad: 83.1\\npoverty: 17.1\\nsingle: 26.3\\nwhite: 79.1\\nurban: 54.38', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 43}, lookup_index=0), Document(page_content='state: Texas\\nviolent: 491.4\\nmurder: 5.4\\nnhs_grad: 79.9\\npoverty: 17.2\\nsingle: 27.6\\nwhite: 73.8\\nurban: 75.35', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 44}, lookup_index=0), Document(page_content='state: Utah\\nviolent: 216.2\\nmurder: 1.4\\nnhs_grad: 90.4\\npoverty: 11.5\\nsingle: 17.9\\nwhite: 89.3\\nurban: 81.17', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 45}, lookup_index=0), Document(page_content='state: Vermont\\nviolent: 135.1\\nmurder: 1.3\\nnhs_grad: 91.0\\npoverty: 11.4\\nsingle: 21.3\\nwhite: 95.8\\nurban: 17.38', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 46}, lookup_index=0), Document(page_content='state: Virginia\\nviolent: 230.0\\nmurder: 4.7\\nnhs_grad: 86.6\\npoverty: 10.5\\nsingle: 24.0\\nwhite: 70.4\\nurban: 69.79', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 47}, lookup_index=0), Document(page_content='state: Washington\\nviolent: 338.3\\nmurder: 2.8\\nnhs_grad: 89.7\\npoverty: 12.3\\nsingle: 22.2\\nwhite: 80.2\\nurban: 74.97', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 48}, lookup_index=0), Document(page_content='state: West Virginia\\nviolent: 331.2\\nmurder: 4.9\\nnhs_grad: 82.8\\npoverty: 17.7\\nsingle: 23.3\\nwhite: 94.3\\nurban: 33.2', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 49}, lookup_index=0), Document(page_content='state: Wisconsin\\nviolent: 259.7\\nmurder: 2.6\\nnhs_grad: 89.8\\npoverty: 12.4\\nsingle: 22.2\\nwhite: 88.4\\nurban: 55.8', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 50}, lookup_index=0), Document(page_content='state: Wyoming\\nviolent: 219.3\\nmurder: 2.0\\nnhs_grad: 91.8\\npoverty: 9.8\\nsingle: 18.9\\nwhite: 91.3\\nurban: 24.51', lookup_str='', metadata={'source': './docs/statecrime.csv', 'row': 51}, lookup_index=0)]\n"
     ]
    }
   ],
   "source": [
    "doc_dir = \"./docs/\"\n",
    "state_crime_csv = doc_dir + \"statecrime.csv\"\n",
    "\n",
    "loader = CSVLoader(file_path = state_crime_csv,\n",
    "                    csv_args = {\"delimiter\":\",\", \"fieldnames\": ['state', 'violent', 'murder', 'nhs_grad', 'poverty', 'single', 'white', 'urban']}\n",
    "                    )\n",
    "data = loader.load()\n",
    "print(data)\n",
    "#fieldnames aids prompting, especially when specifying column needed for results.\n",
    "#Which state is the most unsafe in the united states and what is the murder rate\n",
    "\n",
    "#df_loader = DataFrameLoader(df, page_content_column=\"Team\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -CoRise - Data Centric Deep Learning.pdf\n",
      "46555-3.pdf\n",
      "statecrime.csv\n",
      "Philosophical Investigations.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for item in os.listdir(doc_dir):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='The ML Test Score:\\nA Rubric for ML Production Readiness and Technical Debt Reduction\\nEric Breck, Shanqing Cai, Eric Nielsen, Michael Salib, D. Sculley\\nGoogle, Inc.\\nebreck, cais, nielsene, msalib, dsculley @google.com\\nAbstract —Creating reliable, production-level machine learn-\\ning systems brings on a host of concerns not found in\\nsmall toy examples or even large ofﬂine research experiments.\\nTesting and monitoring are key considerations for ensuring\\nthe production-readiness of an ML system, and for reducing\\ntechnical debt of ML systems. But it can be difﬁcult to formu-\\nlate speciﬁc tests, given that the actual prediction behavior of\\nany given model is difﬁcult to specify a priori . In this paper,\\nwe present 28 speciﬁc tests and monitoring needs, drawn from\\nexperience with a wide range of production ML systems to help\\nquantify these issues and present an easy to follow road-map\\nto improve production readiness and pay down ML technical\\ndebt.\\nKeywords -Machine Learning, Testing, Monitoring, Reliabil-\\nity, Best Practices, Technical Debt\\nI. I NTRODUCTION\\nAs machine learning (ML) systems continue to take on\\never more central roles in real-world production settings,\\nthe issue of ML reliability has become increasingly critical.\\nML reliability involves a host of issues not found in small\\ntoy examples or even large ofﬂine experiments, which can\\nlead to surprisingly large amounts of technical debt [1].\\nTesting and monitoring are important strategies for improv-\\ning reliability, reducing technical debt, and lowering long-\\nterm maintenance cost. However, as suggested by Figure\\n1, ML system testing is also more complex a challenge\\nthan testing manually coded systems, due to the fact that\\nML system behavior depends strongly on data and models\\nthat cannot be strongly speciﬁed a priori . One way to see\\nthis is to consider ML training as analogous to compilation,\\nwhere the source is both code and training data. By that\\nanalogy, training data needs testing like code, and a trained\\nML model needs production practices like a binary does,\\nsuch as debuggability, rollbacks and monitoring.\\nSo, what should be tested and how much is enough?\\nIn this paper, we try to answer this question with a test\\nrubric , which is based on engineering decades of production-\\nlevel ML systems at Google, in systems such as ad click\\nprediction [2] and the Sibyl ML platform [3].\\nWe present a rubric as a set of 28 actionable tests, and\\noffer a scoring system to measure how ready for production\\na given machine learning system is. This rubric is intended\\nto cover a range from a team just starting out with machine\\nlearning up through tests that even a well-established teammay ﬁnd difﬁcult. Note that this rubric focuses on issues\\nspeciﬁc to ML systems, and so does not include generic\\nsoftware engineering best practices such as ensuring good\\nunit test coverage and a well-deﬁned binary release process.\\nSuch strategies remain necessary as well. We do call out\\na few speciﬁc areas for unit or integration tests that have\\nunique ML-related behavior.\\nHow to read the tests: Each test is written as an\\nassertion; our recommendation is to test that the assertion is\\ntrue, the more frequently the better, and to ﬁx the system if\\nthe assertion is not true.\\nDoesn’t this all go without saying?: Before we enu-\\nmerate our suggested tests, we should address one objection\\nthe reader may have – obviously one should write tests for\\nan engineering project! While this is true in principle, in a\\nsurvey of several dozen teams at Google, none of these tests\\nwas implemented by more than 80% of teams (though, even\\nin a engineering culture valuing rigorous testing, many of\\nthese ML-centric tests are non-obvious). Conversely, most\\ntests had a nonzero score for at least half of the teams\\nsurveyed; our tests do represent practices that teams ﬁnd\\nto be worth doing.\\nIn this paper, we are largely concerned with supervised\\nML systems that are trained continuously online and perform\\nrapid, low-latency inference on a server. Features are often\\nderived from large amounts of data such as streaming logs\\nof incoming data. However, most of our recommendations\\napply to other forms of ML systems, such as infrequently\\ntrained models pushed to client-side systems for inference.\\nA. Related work\\nSoftware testing is well studied, as is machine learning,\\nbut their intersection has been less well explored in the\\nliterature. [4] reviews testing for scientiﬁc software more\\ngenerally, and cites a number of articles such as [5], who\\npresent an approach for testing ML algorithms. These ideas\\nare a useful complement for the tests we present, which are\\nfocused on testing the use of ML in a production system\\nrather than just the correctness of the ML algorithm per se.\\nZinkevich provides extensive advice on building effective\\nmachine learning models in real world systems [6]. Those\\nrules are complementary to this rubric, which is more\\nc\\r2017 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media,\\nincluding reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to\\nservers or lists, or reuse of any copyrighted component of this work in other works. Published as [7].', lookup_str='', metadata={'source': './docs/46555-3.pdf', 'page': 0}, lookup_index=0), Document(page_content='Figure 1. ML Systems Require Extensive Testing and Monitoring. The key consideration is that unlike a manually coded system (left), ML-based\\nsystem behavior is not easily speciﬁed in advance. This behavior depends on dynamic qualities of the data, and on various model conﬁguration choices.\\nconcerned with determining how reliable an ML system is\\nrather than how to build one.\\nIssues of surprising sources of technical debt in ML\\nsystems has been studied before [1]. It has been noted that\\nthe prior work has identiﬁed problems but been largely silent\\non how to address them; this paper details actionable advice\\ndrawn from practice and veriﬁed with extensive interviews\\nwith the maintainers of 36 real world systems.\\nII. T ESTS FOR FEATURES AND DATA\\nMachine learning systems differ from traditional software-\\nbased systems in that the behavior of ML systems is not\\nspeciﬁed directly in code but is learned from data. Therefore,\\nwhile traditional software can rely on unit tests and integra-\\ntion tests of the code, here we attempt to add a sufﬁcient\\nset of tests of the data .\\nData 1: Feature expectations are captured in a\\nschema: It is useful to encode intuitions about the data\\nin a schema so they can be automatically checked. For\\nexample, an adult human is surely between one and ten\\nfeet in height. The most common word in English text is\\nprobably ‘the’, with other word frequencies following a\\npower-law distribution. Such expectations can be used for\\ntests on input data during training and serving (see test\\nMonitor 2).\\nHow? To construct the schema, one approach is to start\\nwith calculating statistics from training data, and then ad-\\njusting them as appropriate based on domain knowledge. It\\nmay also be useful to start by writing down expectations\\nand then compare them to the data to avoid an anchoring\\n1 Feature expectations are captured in a schema.\\n2 All features are beneﬁcial.\\n3 No feature’s cost is too much.\\n4 Features adhere to meta-level requirements.\\n5 The data pipeline has appropriate privacy controls.\\n6 New features can be added quickly.\\n7 All input feature code is tested.\\nTable I\\nBRIEF LISTING OF THE SEVEN DATA TESTS .bias. Visualization tools such as Facets1can be very useful\\nfor analyzing the data to produce the schema. Invariants to\\ncapture in a schema can also be inferred automatically from\\nyour system’s behavior [8].\\nData 2: All features are beneﬁcial: A kitchen-sink\\napproach to features can be tempting, but every feature\\nadded has a software engineering cost. Hence, it’s important\\nto understand the value each feature provides in additional\\npredictive power (independent of other features).\\nHow? Some ways to run this test are by computing\\ncorrelation coefﬁcients, by training models with one or two\\nfeatures, or by training a set of models that each have one\\nofkfeatures individually removed.\\nData 3: No feature’s cost is too much: It is not\\nonly a waste of computing resources, but also an ongoing\\nmaintenance burden to include \\x0f-features that add only\\nminimal predictive beneﬁt [1].\\nHow? To measure the costs of a feature, consider not\\nonly added inference latency and RAM usage, but also\\nmore upstream data dependencies, and additional expected\\ninstability incurred by relying on that feature. See Rule#22\\n[6] for further discussion.\\nData 4: Features adhere to meta-level requirements:\\nYour project may impose requirements on the data coming\\nin to the system. It might prohibit features derived from user\\ndata, prohibit the use of speciﬁc features like age, or simply\\nprohibit any feature that is deprecated. It might require all\\nfeatures be available from a single source. However, during\\nmodel development and experimentation, it is typical to try\\nout a wide variety of potential features to improve prediction\\nquality.\\nHow? Programmatically enforce these requirements, so\\nthat all models in production properly adhere to them.\\nData 5: The data pipeline has appropriate privacy\\ncontrols: Training data, validation data, and vocabulary ﬁles\\nall have the potential to contain sensitive user data. While\\nteams often are aware of the need to remove personally iden-\\ntiﬁable information (PII), during this type of exporting and\\n1https://pair-code.github.io/facets/', lookup_str='', metadata={'source': './docs/46555-3.pdf', 'page': 1}, lookup_index=0), Document(page_content='transformations, programming errors and system changes\\ncan lead to inadvertent PII leakages that may have serious\\nconsequences.\\nHow? Make sure to budget sufﬁcient time during new\\nfeature development that depends on sensitive data to allow\\nfor proper handling. Test that access to pipeline data is\\ncontrolled as tightly as the access to raw user data, especially\\nfor data sources that haven’t previously been used in ML.\\nFinally, test that any user-requested data deletion propagates\\nto the data in the ML training pipeline, and to any learned\\nmodels.\\nData 6: New features can be added quickly: The\\nfaster a team can go from a feature idea to the feature\\nrunning in production, the faster it can both improve the\\nsystem and respond to external changes. For highly efﬁcient\\nteams, this can be as little as one to two months even for\\nglobal-scale, high-trafﬁc ML systems. Note that this can\\nbe in tension with Data 5, but privacy should always take\\nprecedence.\\nData 7: All input feature code is tested: Feature\\ncreation code may appear simple enough to not need unit\\ntests, but this code is crucial for correct behavior and so\\nits continued quality is vital. Bugs in features may be\\nalmost impossible to detect once they have entered the data\\ngeneration process, especially if they are represented in both\\ntraining and test data.\\nIII. T ESTS FOR MODEL DEVELOPMENT\\nWhile the ﬁeld of software engineering has developed a\\nfull range of best practices for developing reliable software\\nsystems, similar best-practices for ML model development\\nare still emerging.\\nModel 1: Every model speciﬁcation undergoes a\\ncode review and is checked in to a repository: It can\\nbe tempting to avoid code review out of expediency, and\\nrun experiments based on one’s own personal modiﬁcations.\\nIn addition, when responding to production incidents, it’s\\ncrucial to know the exact code that was run to produce a\\ngiven learned model. For example, a responder might need\\nto re-run training with corrected input data, or compare the\\nresult of a particular modiﬁcation. Proper version control of\\nthe model speciﬁcation can help make training auditable and\\nimprove reproducibility.\\n1 Model specs are reviewed and submitted.\\n2 Ofﬂine and online metrics correlate.\\n3 All hyperparameters have been tuned.\\n4 The impact of model staleness is known.\\n5 A simpler model is not better.\\n6 Model quality is sufﬁcient on important data slices.\\n7 The model is tested for considerations of inclusion.\\nTable II\\nBRIEF LISTING OF THE SEVEN MODEL TESTSModel 2: Ofﬂine proxy metrics correlate with actual\\nonline impact metrics: A user-facing production system’s\\nimpact is judged by metrics of engagement, user happiness,\\nrevenue, and so forth. A machine learning system is trained\\nto optimize loss metrics such as log-loss or squared error.\\nA strong understanding of the relationship between these\\nofﬂine proxy metrics and the actual impact metrics is needed\\nto ensure that a better scoring model will result in a better\\nproduction system.\\nHow? The ofﬂine/online metric relationship can be mea-\\nsured in one or more small scale A/B experiments using an\\nintentionally degraded model.\\nModel 3: All hyperparameters have been tuned:\\nA ML model can often have multiple hyperparameters,\\nsuch as learning rates, number of layers, layer sizes and\\nregularization coefﬁcients. Choice of the hyperparameter\\nvalues can have dramatic impact on prediction quality.\\nHow? Methods such as a grid search [9] or a more\\nsophisticated hyperparameter search strategy [10] [11] not\\nonly improve prediction quality, but also can uncover hid-\\nden reliability issues. Substantial performance improvements\\nhave been realized in many ML systems through use of an\\ninternal hyperparameter tuning service[12]2.\\nModel 4: The impact of model staleness is known:\\nMany production ML systems encounter rapidly changing,\\nnon-stationary data. Examples include content recommen-\\ndation systems and ﬁnancial ML applications. For such\\nsystems, if the pipeline fails to train and deploy sufﬁciently\\nup-to-date models, we say the model is stale . Understanding\\nhow model staleness affects the quality of predictions is\\nnecessary to determine how frequently to update the model.\\nIf predictions are based on a model trained yesterday versus\\nlast week versus last year, what is the impact on the\\nlive metrics of interest? Most models need to be updated\\neventually to account for changes in the external world;\\na careful assessment is important to decide how often to\\nperform the updates (see Rule 8 in [6] for related discussion).\\nHow? One way of testing the impact of staleness is with\\na small A/B experiment with older models. Testing a range\\nof ages can provide an age-versus-quality curve to help\\nunderstand what amount of staleness is tolerable.\\nModel 5: A simpler model is not better: Regularly\\ntesting against a very simple baseline model, such as a linear\\nmodel with very few features, is an effective strategy both\\nfor conﬁrming the functionality of the larger pipeline and\\nfor helping to assess the cost to beneﬁt tradeoffs of more\\nsophisticated techniques.\\nModel 6: Model quality is sufﬁcient on all important\\ndata slices: Slicing a data set along certain dimensions of\\ninterest can improve ﬁne-grained understanding of model\\nquality. Slices should distinguish subsets of the data that\\nmight behave qualitatively differently, for example, users by\\n2The service is closely related to HyperTune[13].', lookup_str='', metadata={'source': './docs/46555-3.pdf', 'page': 2}, lookup_index=0), Document(page_content='Table III\\nBRIEF LISTING OF THE ML I NFRASTRUCTURE TESTS\\n1 Training is reproducible.\\n2 Model specs are unit tested.\\n3 The ML pipeline is Integration tested.\\n4 Model quality is validated before serving.\\n5 The model is debuggable.\\n6 Models are canaried before serving.\\n7 Serving models can be rolled back.\\ncountry, users by frequency of use, or movies by genre.\\nExamining sliced data avoids having ﬁne-grained quality\\nissues masked by a global summary metric, e.g. global\\naccuracy improved by 1% but accuracy for one country\\ndropped by 50%. This class of problems often arises from\\na fault in the collection of training data, that caused an\\nimportant set of training data to be lost or late.\\nHow? Consider including these tests in your release\\nprocess, e.g. release tests for models can impose absolute\\nthresholds (e.g., error for slice xmust be<5%), to catch\\nlarge drops in quality, as well as incremental (e.g. the change\\nin error for slice xmust be<1% compared to the previously\\nreleased model).\\nModel 7: The model has been tested for considera-\\ntions of inclusion: There have been a number of recent\\nstudies on the issue of ML Fairness [14], [15], which\\nmay arise inadvertently due to factors such as choice of\\ntraining data. For example, Bolukbasi et al. found that a\\nword embedding trained on news articles had learned some\\nstriking associations between gender and occupation that\\nmay have reﬂected the content of the news articles but\\nwhich may have been inappropriate for use in a predictive\\nmodeling context [14]. This form of potentially overlooked\\nbiases in training data sets may then inﬂuence the larger\\nsystem behavior.\\nHow? Diagnosing such issues is an important step for\\ncreating robust modeling systems that serve all users well.\\nTests that can be run include examining input features to\\ndetermine if they correlate strongly with protected user\\ncategories, and slicing predictions to determine if prediction\\noutputs differ materially when conditioned on different user\\ngroups.\\nBolukbasi et al. [14] propose one method for ameliorating\\nsuch effects by projecting embeddings to spaces that collapse\\ndifferences along certain protected dimensions. Hardt et al\\npropose a post-processing step in model creation to mini-\\nmize disproportionate loss for certain groups in the manner\\nof [15]. Finally, the approach of collecting more data to\\nensure data representation for potentially under-represented\\ncategories or subgroups can be effective in many cases.\\nIV. T ESTS FOR ML I NFRASTRUCTURE\\nAn ML system often relies on a complex pipeline rather\\nthan a single running binary.Infra 1: Training is reproducible: Ideally, training\\ntwice on the same data should produce two identical mod-\\nels. Deterministic training dramatically simpliﬁes reasoning\\nabout the whole system and can aid auditability and debug-\\nging. For example, optimizing feature generation code is a\\ndelicate process but verifying that the old and new feature\\ngeneration code will train to an identical model can provide\\nmore conﬁdence that the refactoring was correct. This sort\\nof diff-testing relies entirely on deterministic training.\\nUnfortunately, model training is often not reproducible in\\npractice, especially when working with non-convex methods\\nsuch as deep learning or even random forests. This can\\nmanifest as a change in aggregate metrics across an entire\\ndataset, or, even if the aggregate performance appears the\\nsame from run to run, as changes on individual examples.\\nRandom number generation is an obvious source of non-\\ndeterminism, which can be alleviated with seeding. But\\neven with proper seeding, initialization order can be un-\\nderspeciﬁed so that different portions of the model will be\\ninitialized at different times on different runs leading to\\nnon-determinism. Furthermore, even when initialization is\\nfully deterministic, multiple threads of execution on a single\\nmachine or across a distributed system [16] may be subject\\nto unpredictable orderings of training data, which is another\\nsource of non-determinism.\\nHow? Besides working to remove nondeterminism as\\ndiscussed above, ensembling models can help.\\nInfra 2: Model speciﬁcation code is unit tested: Al-\\nthough model speciﬁcations may seem like “conﬁguration”,\\nsuch ﬁles can have bugs and need to be tested. Unfortunately,\\ntesting a model speciﬁcation can be very hard. Unit tests\\nshould run quickly and require no external dependencies but\\nmodel training is often a very slow process that involves\\npulling in lots of data from many sources.\\nHow? It’s useful to distinguish two kinds of model tests:\\ntests of API usage and tests of algorithmic correctness. We\\nplan to release an open source framework implementing\\nsome of these tests soon.\\nML APIs can be complex, and code using them can\\nbe wrong in subtle ways. Even if code errors would be\\napparent after training (due to a model that fails to train\\nor results in poor performance), training is expensive and\\nso the development loop is slow. We have found in practice\\nthat a simple unit test to generate random input data, and\\ntrain the model for a single step of gradient descent is quite\\npowerful for detecting a host of common library mistakes,\\nresulting in a much faster development cycle. Another useful\\nassertion is that a model can restore from a checkpoint after\\na mid-training job crash.\\nTesting correctness of a novel implementation of an ML\\nalgorithm is more difﬁcult, but still necessary – it is not\\nsufﬁcient that code produces a model with high quality\\npredictions, but that it does so for the expected reasons. One\\nsolution is to make assertions that speciﬁc subcomputations', lookup_str='', metadata={'source': './docs/46555-3.pdf', 'page': 3}, lookup_index=0), Document(page_content='of the algorithm are correct, e.g. that a speciﬁc part of\\nan RNN was executed exactly once per element of the\\ninput sequence. Another solution involves not training to\\ncompletion in the unit test but only training for a few\\niterations and verifying that loss decreases with training. Still\\nanother is to purposefully train a model for overﬁtting: if one\\ncan get a model to effectively memorize its training data,\\nthen that provides some conﬁdence that learning reliably\\nhappens. When testing models, pains should be taken to\\navoid “golden tests”, i.e., tests that partially train a model\\nand compare the results to a previously generated model –\\nsuch tests are difﬁcult to maintain over time without blindly\\nupdating the golden ﬁle. In addition to problems in training\\nnon-determinism, when these tests do break they provide\\nvery little insight into how or why. Additionally, ﬂaky tests\\nremain a real danger here.\\nInfra 3: The full ML pipeline is integration tested:\\nA complete ML pipeline typically consists of assembling\\ntraining data, feature generation, model training, model\\nveriﬁcation, and deployment to a serving system. Although\\na single engineering team may be focused on a small part\\nof the process, each stage can introduce errors that may\\naffect subsequent stages, possibly even several stages away.\\nThat means there must be a fully automated test that runs\\nregularly and exercises the entire pipeline, validating that\\ndata and code can successfully move through each stage\\nand that the resulting model performs well.\\nHow? The integration test should run both continuously\\nas well as with new releases of models or servers, in order\\nto catch problems well before they reach production. Faster\\nrunning integration tests with a subset of training data or a\\nsimpler model can give faster feedback to developers while\\nstill backed by less frequent, long running versions with a\\nsetup that more closely mirrors production.\\nInfra 4: Model quality is validated before attempting\\nto serve it: After a model is trained but before it actually\\naffects real trafﬁc, an automated system needs to inspect\\nit and verify that its quality is sufﬁcient; that system must\\neither bless the model or veto it, terminating its entry to the\\nproduction environment.\\nHow? It is important to test for both slow degradations\\nin quality over many versions as well as sudden drops in\\na new version. For the former, setting loose thresholds and\\ncomparing against predictions on a validation set can be\\nuseful; for the latter, it is useful to compare predictions\\nto the previous version of the model while setting tighter\\nthresholds.\\nInfra 5: The model allows debugging by observing\\nthe step-by-step computation of training or inference on\\na single example: When someone ﬁnds a case where a\\nmodel is behaving bizarrely, how difﬁcult is it to ﬁgure\\nout why? Is there an easy, well documented process for\\nfeeding a single example to the model and investigating\\nthe computation through each stage of the model (e.g. eachFigure 2. Importance of a Model Canary before Serving. It is possible\\nfor models to incorporate new pieces of code that are not live in separate\\nserving binaries, causing havoc at serving time. Using small scale canary\\nprocesses can help protect against this.\\ninternal node of a neural network)?\\nObserving the step-by-step computation through the\\nmodel on small amounts of data is an especially useful\\ndebugging strategy for issues like numerical instability.\\nHow? An internal tool that allows users to enter examples\\nand see how the a speciﬁc model version interprets it can be\\nvery helpful. The TensorFlow debugger [17] is one example\\nof such a tool.\\nInfra 6: Models are tested via a canary process\\nbefore they enter production serving environments:\\nOfﬂine testing, however extensive, cannot by itself guarantee\\nthe model will perform well in live production settings,\\nas the real world often contains signiﬁcant non-stationarity\\nor other issues that limit the utility of historical data.\\nConsequently, there is always some risk when turning on\\na new model in production.\\nOne recurring problem that canarying can help catch\\nis mismatches between model artifacts and serving infras-\\ntructure. Modeling code can change more frequently than\\nserving code, so there is a danger that an older serving\\nsystem will not be able to serve a model trained from newer\\ncode. For example, as shown in Figure 2, a refactoring\\nin the core learning library might change the low-level\\nimplementation of an operation Opin the model from Op0.1\\nto a more efﬁcient implementation, Op0.2 . A newly trained\\nmodel will thus expect to be implemented with Op0.2 ; an\\nolder deployed server will not include Op0.2 and so will\\nrefuse to load the model.\\nHow? To mitigate the mismatch issue, one approach\\nis testing that a model successfully loads into production\\nserving binaries and that inference on production input data\\nsucceeds. To mitigate the new-model risk more generally,\\none can turn up new models gradually, running old and new\\nmodels concurrently, with new models only seeing a small\\nfraction of trafﬁc, gradually increased as the new model is\\nobserved to behave sanely.\\nInfra 7: Models can be quickly and safely rolled\\nback to a previous serving version: A model “roll back”\\nprocedure is a key part of incident response to many of\\nthe issues that can be detected by the monitoring discussed\\nin Section V. Being able to quickly revert to a previous\\nknown-good state is as crucial with ML models as with any\\nother aspect of a serving system. Because rolling back is\\nan emergency procedure, operators should practice doing it', lookup_str='', metadata={'source': './docs/46555-3.pdf', 'page': 4}, lookup_index=0), Document(page_content='Table IV\\nBRIEF LISTING OF THE SEVEN MONITORING TESTS\\n1 Dependency changes result in notiﬁcation.\\n2 Data invariants hold for inputs.\\n3 Training and serving are not skewed.\\n4 Models are not too stale.\\n5 Models are numerically stable.\\n6 Computing performance has not regressed.\\n7 Prediction quality has not regressed.\\nnormally, when not in emergency conditions.\\nV. M ONITORING TESTS FOR ML\\nIt is crucial to know not just that your ML system worked\\ncorrectly at launch, but that it continues to work correctly\\nover time. An ML system by deﬁnition is making predictions\\non previously unseen data, and typically also incorporates\\nnew data over time into training. The standard approach\\nis to monitor the system, i.e. to have a constantly-updated\\n“dashboard” user interface displaying relevant graphs and\\nstatistics, and to automatically alert the engineering team\\nwhen particular metrics deviate signiﬁcantly from expecta-\\ntions. For ML systems, it is important to monitor serving\\nsystems, training pipelines, and input data. Here we rec-\\nommend speciﬁc metrics to monitor throughout the system.\\nThe usual sorts of incident response approaches will apply;\\none unique to ML is to roll back not the system code but\\nthe learned model, hence our test earlier (test Infra 7) to\\nregularly ensure that this process is safe and easy.\\nMonitor 1: Dependency changes result in notiﬁca-\\ntion: ML systems typically consume data from a wide array\\nof other systems to generate useful features. Partial outages,\\nversion upgrades, and other changes in the source system can\\nradically change the feature’s meaning and thus confuse the\\nmodel’s training or inference, without necessarily producing\\nvalues that are strange enough to trigger other monitoring.\\nHow? Make sure that your team is subscribed to and reads\\nannouncement lists for all dependencies, and make sure that\\nthe dependent team knows your team is using the data.\\nMonitor 2: Data invariants hold in training and\\nserving inputs: It can be difﬁcult to effectively monitor\\nthe internal behavior of a learned model for correctness, but\\nthe input data should be more transparent. Consequently,\\nanalyzing and comparing data sets is the ﬁrst line of defense\\nfor detecting problems where the world is changing in ways\\nthat can confuse an ML system.\\nHow? Using the schema constructed in test Data 1,\\nmeasure whether data matches the schema and alert when\\nthey diverge signiﬁcantly. In practice, careful tuning of\\nalerting thresholds is needed to achieve a useful balance\\nbetween false positive and false negative rates to ensure these\\nalerts remain useful and actionable.\\nMonitor 3: Training and serving features compute\\nthe same values: The codepaths that actually generate input\\nfeatures may differ at training and inference time. IdeallyFigure 3. Monitoring for Training/Serving Skew. It is often necessary\\nfor the same feature to be computed in different ways in different parts\\nof the system. In such cases, we must carefully test that these different\\ncodepaths are in fact logically identical.\\nthe different codepaths should generate the same values, but\\nin practice a common problem is that they do not. This\\nis sometimes called “training/serving skew” and requires\\ncareful monitoring to detect and avoid. As one concrete\\nexample, imagine adding a new feature to an existing\\nproduction system. While the value of the feature in the\\nserving system might be computed based on data from live\\nuser behavior, the feature will not be present in training data,\\nand so must be backﬁlled by imputing it from other stored\\ndata, likely using an entirely independent codepath. Another\\nexample is when the computation at training time is done\\nusing code that is highly ﬂexible (for easy experimentation)\\nbut inefﬁcient, while at serving time the same computation\\nis heavily optimized for low latency.\\nHow? To measure this, it is crucial to log a sample of\\nactual serving trafﬁc. For systems that use serving input as\\nfuture training data, adding identiﬁers to each example at\\nserving time will allow direct comparison; the feature values\\nshould be perfectly identical at training and serving time for\\nthe same example. Important metrics to monitor here are\\nthe number of features that exhibit skew, and the number of\\nexamples exhibiting skew for each skewed feature.\\nAnother approach is to compute distribution statistics\\non the training features and the sampled serving features,\\nand ensure that they match. Typical statistics include the\\nminimum, maximum, or average, values, the fraction of\\nmissing values, etc. Again, thresholds for alerting on these\\nmetrics must be carefully tuned to ensure a low enough false\\npositive rate for actionable response.\\nMonitor 4: Models are not too stale: In test Model 4\\nwe discussed testing the effect that an old (“stale”) model has\\non prediction quality. Here, we recommend monitoring how\\nold the system in production is, using the prior measurement\\nas a guide for determining what age is problematic enough\\nto raise an alert.\\nSurprisingly, infrequently updated models also incur a\\nmaintenance cost. Imagine a model that is manually re-\\ntrained once or twice a year by a given engineer. If that\\nengineer leaves the team, this process may be difﬁcult to\\nreplicate – even carefully written instructions may become\\nstale or incorrect over this kind of time horizon.', lookup_str='', metadata={'source': './docs/46555-3.pdf', 'page': 5}, lookup_index=0), Document(page_content='How? For models that re-train regularly (e.g. weekly\\nor more often), the most obvious metric is the age of the\\nmodel in production. It is also important to measure the age\\nof the model at each stage of the training pipeline, to quickly\\ndetermine where a stall has occurred and react appropriately.\\nEven for models that re-train more infrequently, there\\nis often a dependence on data aggregation or other such\\nprocesses to produce features, which can themselves grow\\nstale. For example, consider using a feature based on the\\nmost popular nitems (movies, apps, cars, etc). The process\\nthat computes the top- ntable must be re-run frequently, and\\nit is crucial to monitor the age of this table, so that if the\\nprocess stops running, alerts will ﬁre.\\nMonitor 5: The model is numerically stable:\\nInvalid or implausible numeric values can potentially crop\\nup during model training without triggering explicit errors,\\nand knowing that they have occurred can speed diagnosis of\\nthe problem.\\nHow? Explicitly monitor the initial occurrence of any\\nNaNs or inﬁnities. Set plausible bounds for weights and\\nthe fraction of ReLU units in a layer returning zero values,\\nand trigger alerts during training if these exceed appropriate\\nthresholds.\\nMonitor 6: The model has not experienced a dra-\\nmatic or slow-leak regressions in training speed, serving\\nlatency, throughput, or RAM usage: The computational\\nperformance (as opposed to predictive quality) of an ML\\nsystem is often a key concern at scale. Deep neural networks\\ncan be slow to train and run inference on, wide linear models\\nwith feature crosses can use a lot of memory; any ML model\\nmay take days to train; and so forth. Swiftly reacting to\\nchanges in this performance due to changes in data, features,\\nmodeling, or underlying compute library or infrastructure is\\ncrucial to maintaining a performant system.\\nHow? While measuring computational performance is\\na standard part of any monitoring, it is useful to slice\\nperformance metrics not just by the versions and components\\nof code, but also by data and model versions. Degradations\\nin computational performance may occur with dramatic\\nchanges (for which comparison to performance of prior\\nversions or time slices can be helpful for detection) or in\\nslow leaks (for which a pre-set alerting threshold can be\\nhelpful for detection)\\nMonitor 7: The model has not experienced a re-\\ngression in prediction quality on served data: Validation\\ndata will always be older than real serving input data, so\\nmeasuring a model’s quality on that validation data before\\npushing it to serving is only an estimate of quality metrics on\\nactual live serving inputs. However, it is not always possible\\nto know the correct labels even shortly after serving time,\\nmaking quality measurement difﬁcult.\\nHow? Here are some options to make sure that there is\\nno degradation in served prediction quality due to changes\\nin data, differing codepaths, etc.\\x0fMeasure statistical bias in predictions, i.e. the average\\nof predictions in a particular slice of data. Generally\\nspeaking, models should have zero bias, in aggregate\\nand on slices (e.g. 90% of predictions of probability\\n0.9 should in fact be positive). Knowing that a model\\nis unbiased is not enough to know it is any good, but\\nknowing there is bias can be a useful canary to detect\\nproblems.\\n\\x0fIn some tasks, the label actually is available immedi-\\nately or soon after the prediction is made (e.g. will a\\nuser click on an ad). In this case, we can judge the\\nquality of predictions in almost real-time and identify\\nproblems quickly.\\n\\x0fFinally, it can be useful to periodically add new training\\ndata by having human raters manually annotate labels\\nfor logged serving inputs. Some of this data can be held\\nout to validate the served predictions.\\nHowever the measure can be done, thresholds must be\\nset as to acceptable quality (e.g. based on bounds of quality\\nat the launch of the initial system), and then a responder\\nshould be notiﬁed immediately if quality drifts outside\\nthat threshold. As with computational performance, it is\\ncrucial to monitor both dramatic and slow-leak regressions\\nin prediction quality.\\nVI. I NCENTIVIZING CULTURE CHANGE\\nBecause technical debt is difﬁcult to quantify, it can be\\ndifﬁcult to prioritize paydown or measure improvements.\\nTo address this, our rubric provides a quantiﬁed ML Test\\nScore which can be measured and improved over time. This\\nprovides a vector for incentivizing ML system developers to\\nachieve strong levels of reliability by providing a clear indi-\\ncator of readiness and clear guidelines for how to improve.\\nThis strategy was inspired by the Test Certiﬁed program\\nat Google, which provided a scored ladder for overall test\\nrobustness, and which had strong success in incentivizing\\nteams to adopt best practices.\\nA. Computing an ML Test Score\\nThe ﬁnal test score is computed as follows:\\n\\x0fFor each test, half a point is awarded for executing the\\ntest manually, with the results documented and distributed.\\n\\x0fA full point is awarded if there is a system in place to\\nrun that test automatically on a repeated basis.\\n\\x0fSum the score for each of the 4 sections individually.\\n\\x0fThe ﬁnal ML Test Score is computed by taking the\\nminimum of the scores aggregated for each of the 4 sections.\\nWe choose the minimum because we believe all four\\nsections are important, and so a system must consider all\\nin order to raise the score. One downside of this approach\\nis that it reduces the extent to which an individual’s efforts\\nare reﬂected in higher system scores and ranks; it remains\\nto be seen how this will affect the adoption of our system.', lookup_str='', metadata={'source': './docs/46555-3.pdf', 'page': 6}, lookup_index=0), Document(page_content='Points Description\\n0 More of a research project than a productionized system.\\n(0,1] Not totally untested, but it is worth considering the possibility of serious holes in reliability.\\n(1,2] There’s been ﬁrst pass at basic productionization, but additional investment may be needed.\\n(2,3] Reasonably tested, but it’s possible that more of those tests and procedures may be automated.\\n(3,5] Strong levels of automated testing and monitoring, appropriate for mission-critical systems.\\n>5 Exceptional levels of automated testing and monitoring.\\nTable V\\nInterpreting an ML Test Score. THIS SCORE IS COMPUTED BY TAKING THE minimum SCORE FROM EACH OF THE FOUR TEST AREAS . NOTE THAT\\nDIFFERENT SYSTEMS AT DIFFERENT POINTS IN THEIR DEVELOPMENT MAY REASONABLY AIM TO BE AT DIFFERENT POINTS ALONG THIS SCALE .\\nAll tests are worth the same number of points. This is\\nintentional, as we believe the relative importance of tests\\nto teams will vary depending on their speciﬁc priorities.\\nThis means that choosing any test to implement will raise\\nthe score, and we feel that is appropriate, as they are each\\nvaluable and often working on one will make it easier to\\nwork on another.\\nTo interpret the score, see Table V. These interpretations\\nwere calibrated against a number of internal ML systems,\\nand overall have been reﬂective of other qualitative percep-\\ntions of those systems.\\nVII. A PPLYING THE RUBRIC TO REAL SYSTEMS\\nWe developed the ML Test Certiﬁed program to help\\nengineers doing ML work at Google. Some of our work has\\ninvolved meeting with teams doing ML and evaluating their\\nperformance in a structured interview based on the rubric\\ndetailed above. We met with 36 teams from across Google\\nworking in a diverse array of product areas; their scores on\\nthe rubric are presented in Figure 4. These interviews have\\noffered some unexpected insights.\\nA. The importance of checklists\\nChecklists are helpful even for expert teams [18]. For\\nexample, one team we worked with discovered a thousand-\\nline code ﬁle, completely untested, that created their input\\nfeatures. Code of that size, even if it contains only simple\\nand straightforward logic, will likely have bugs, against\\nwhich simple unit tests can provide an effective hedge.\\nAnother example we found was a team who realized when\\nwe asked that they had no evaluation or monitoring to\\ndiscover if their global service was serving poor predictions\\nlocalized to a single country. They also relied heavily on\\ninformal evaluation of performance based on the team’s own\\nusage of the product, which does not protect users very\\ndifferent from the team members. Similarly, the interviews\\nwere useful simply as a way of advertising the existing tools\\n– some teams had not even heard of the Facets tools or of\\nour unit testing framework mentioned in Infra 2.\\nAs another example, when we asked one team about ML\\ninclusiveness, they conﬁdently answered that they had given\\nthe matter some thought and concluded that there was no\\nway for their system to be biased since they were only\\ndealing with speech waveforms (“we just get vectors ofnumbers”). When we asked if they had done any work to\\nensure their system performed well for African American\\nVernacular English or had taken steps to ensure diversity\\nin the population of human raters they hired for scoring,\\nthey paused at length and then agreed that this question\\nopened up new possibilities for debiasing which they had\\nnot considered and would address.\\nFinally, the context of our interview provided additional\\nmotivation for getting around to implementing tests - one\\nteam was motivated to implement feature code tests because\\nof the clear danger of training/serving skew, while others\\nwere spurred to automate previously manual processes to\\nmake them more frequent and testable.\\nB. Dependency issues\\nData dependencies can lead to outsourcing responsibility\\nfor fully understanding it. Multiple teams initially suggested\\nthat since their features were produced by an upstream,\\nmuch larger service, any problems in their data would be\\ndiscovered by the other team. While this can certainly be\\nsome protection, it may still be that the smaller team has\\ndifferent requirements for the data that would not be caught\\nby the larger team’s validation.\\nIn the other direction, multiple teams initially suggested\\nthat their system did not require independent monitoring, as\\ntheir serving was done via a larger system whose reliability\\nengineers would notice any problems downstream. Again,\\nthis can be some protection, but it’s also quite possible that\\nthe smaller system’s errors may be masked in the noise of\\nthe larger system. In addition, it’s crucial in that regime that\\nthe larger system know how to ﬁnd the appropriate contact\\nperson from the smaller one.\\nFor the data tests, several teams indicated a key distinction\\nbetween features that represent new combinations of existing\\ndata sources, and features based on new data sources. The\\nlatter requires signiﬁcantly more time and introduces more\\nrisk. Depending on a new data source can mean time spent\\nnegotiating with the owning team to ensure the data is\\nproperly treated. Or if the data come from newly logged\\ninformation, the existing training data must be backﬁlled, or\\nthrown away to wait for new logs including the data.\\nC. The importance of frameworks\\nIntegration testing (Infra 3) stood out as a test with\\nmuch lower adoption than most. When implemented, it often', lookup_str='', metadata={'source': './docs/46555-3.pdf', 'page': 7}, lookup_index=0), Document(page_content='Figure 4. Average scores for interviewed teams. These graphs display the average score for each test across the 36 systems we examined.\\nincluded serving systems but not training. This is in part\\nbecause training is often developed as an ad hoc set of scripts\\nand manual processes. A training pipeline platform like the\\nTFX system[19] can be beneﬁcial here as it then allows\\nbuilding a generic integration test.\\nModel canarying (Infra 6) was frequently implemented by\\nmany teams, and cited as a key part of their testing plan. But\\nthis masks two interesting issues. First, canarying can indeed\\ncatch many issues like unservable models, numeric instabil-\\nity, and so forth. However, it typically occurs long after the\\nengineering decisions that led to the issue, so it would be\\nmuch preferable to catch issues earlier in unit or integration\\ntests. Second, the teams that implemented canarying usually\\ndid so because their existing release framework made it easy\\n– and one team lacking such a framework reported the one\\ntime they did canary it was so painful they’d never do it\\nagain.\\nPerhaps the most important and least implemented test\\nis the one for training/serving skew (Monitor 3). This sort\\nof error is responsible for production issues across a wide\\nswath of teams, and yet it is one of the least frequentlyimplemented tests. In part this is because it is difﬁcult, but\\nagain, building this into a framework like TFX allows many\\nteams to beneﬁt from a single investment.\\nTo test TFX, we evaluated a hypothetical system that\\nused TFX along with its standard recommendations for\\nintroductory data analysis and so forth. We found that this\\nhypothetical system already scored as “reasonably tested”\\naccording to our criterion. TFX is quite new, however, and\\nwe haven’t yet measured real world TFX systems.\\nD. Assessing the assessment\\nWe also conducted some meta-level assessment, asking\\nteams what was useful or non-useful about this rubric.\\nOne interesting theme was that teams using purely image\\nor audio data did not feel many of the Data tests were\\napplicable. However, methods like manual inspection of raw\\ndata and LIME-style importance analysis [20] remain im-\\nportant tools in such settings. For example, such inspection\\ncan reveal skew in distributions or unrealistically consistent\\nbackground effects correlated with the training target.\\nSupervised ML requires labeled data, but a number of\\ngroups are working in domains where labels are either not', lookup_str='', metadata={'source': './docs/46555-3.pdf', 'page': 8}, lookup_index=0), Document(page_content='present or extremely expensive to acquire. One group had\\nan extremely large data set that was so diverse that using\\nhuman raters to generate training labels proved infeasible.\\nSo they built a simple heuristic system and then used that to\\ntrain an ML system (“The ML experts told us that training\\na model like this was crazy and would never work but they\\nwere wrong!”). Human raters consistently rate the heuristic\\nsystem as good but the ML system trained from it as much\\nbetter – however, this exposes a need for a level of testing\\nof the base heuristic system that is not covered in our\\nrubric. Expensive labels also mean that quality evaluation\\nof a learned model is difﬁcult, which impacts the ability of\\nteams to implement several tests like Model 4 and Infra 4.\\nACKNOWLEDGMENT\\nWe are very grateful to Keith Arner, Gary Holt, Josh\\nLovejoy, Fernando Pereira, Todd Phillips, Tal Shaked, Todd\\nUnderwood, Martin Wicke, Cory Williams, and Martin\\nZinkevich for many helpful discussions and comments on\\ndrafts of this paper.\\nREFERENCES\\n[1] D. Sculley, G. Holt, D. Golovin, E. Davydov, T. Phillips,\\nD. Ebner, V . Chaudhary, and M. Young, “Machine learning:\\nThe high interest credit card of technical debt,” in SE4ML:\\nSoftware Engineering for Machine Learning (NIPS 2014\\nWorkshop) , 2014.\\n[2] H. B. McMahan, G. Holt, D. Sculley, M. Young, D. Ebner,\\nJ. Grady, L. Nie, T. Phillips, E. Davydov, D. Golovin,\\nS. Chikkerur, D. Liu, M. Wattenberg, A. M. Hrafnkelsson,\\nT. Boulos, and J. Kubica, “Ad click prediction: A view\\nfrom the trenches,” in Proceedings of the 19th ACM\\nSIGKDD International Conference on Knowledge Discovery\\nand Data Mining , ser. KDD ’13. New York, NY ,\\nUSA: ACM, 2013, pp. 1222–1230. [Online]. Available:\\nhttp://doi.acm.org/10.1145/2487575.2488200\\n[3] T. Chandra, E. Ie, K. Goldman, T. L. Llinares, J. McFadden,\\nF. Pereira, J. Redstone, T. Shaked, and Y . Singer, “Sibyl: a\\nsystem for large scale machine learning,” vol. 28, Jul. 2010.\\n[4] U. Kanewala and J. M. Bieman, “Testing scientiﬁc software:\\nA systematic literature review,” Information and software\\ntechnology , vol. 56, no. 10, pp. 1219–1232, 2014.\\n[5] C. Murphy, G. E. Kaiser, and M. Arias, “An approach to\\nsoftware testing of machine learning applications.” in SEKE .\\nCiteseer, 2007, p. 167.\\n[6] M. Zinkevich, “Rules of machine learning,” Invited talk\\nat the NIPS Reliable Machine Learning Workshop, 1996.\\n[Online]. Available: http://martin.zinkevich.org/rules ofml/\\nrules ofml.pdf\\n[7] E. Breck, S. Cai, E. Nielsen, M. Salib, and D. Sculley, “The\\nML test score: A rubric for ML production readiness and\\ntechnical debt reduction,” in Proceedings of IEEE Big Data\\n2017 , 2017.[8] M. D. Ernst, J. H. Perkins, P. J. Guo, S. McCamant,\\nC. Pacheco, M. S. Tschantz, and C. Xiao, “The daikon\\nsystem for dynamic detection of likely invariants,” Science\\nof Computer Programming , vol. 69, no. 1, pp. 35–45, 2007.\\n[9] C.-W. Hsu, C.-C. Chang, C.-J. Lin et al. , “A practical guide\\nto support vector classiﬁcation,” 2003.\\n[10] J. Snoek, H. Larochelle, and R. P. Adams, “Practical bayesian\\noptimization of machine learning algorithms,” in Advances in\\nneural information processing systems , 2012.\\n[11] T. Desautels, A. Krause, and J. Burdick, “Parallelizing\\nexploration-exploitation tradeoffs in gaussian process ban-\\ndit optimization,” Journal of Machine Learning Research\\n(JMLR) , vol. 15, p. 40534103, December 2014.\\n[12] D. Golovin, B. Solnik, S. Moitra, G. Kochanski, J. Karro,\\nand D. Sculley, “Google vizier: A service for black-box\\noptimization,” in KDD 2017 , 2017.\\n[13] “Google cloud machine learning: now open to all with new\\nprofessional services and education programs,” https://goo.gl/\\nULh7ZW, 2017, accessed: 2017-02-08.\\n[14] T. Bolukbasi, K.-W. Chang, J. Y . Zou, V . Saligrama, and\\nA. T. Kalai, “Man is to computer programmer as woman\\nis to homemaker? debiasing word embeddings,” in Advances\\nin Neural Information Processing Systems 29 , D. D. Lee,\\nM. Sugiyama, U. V . Luxburg, I. Guyon, and R. Garnett, Eds.,\\n2016.\\n[15] M. Hardt, E. Price, N. Srebro et al. , “Equality of opportunity\\nin supervised learning,” in Advances in Neural Information\\nProcessing Systems , 2016, pp. 3315–3323.\\n[16] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin,\\nM. Mao, M. aurelio Ranzato, A. Senior, P. Tucker, K. Yang,\\nQ. V . Le, and A. Y . Ng, “Large scale distributed deep\\nnetworks,” in Advances in Neural Information Processing\\nSystems 25 , F. Pereira, C. J. C. Burges, L. Bottou, and\\nK. Q. Weinberger, Eds. Curran Associates, Inc., 2012, pp.\\n1223–1231. [Online]. Available: http://papers.nips.cc/paper/\\n4687-large-scale-distributed-deep-networks.pdf\\n[17] S. Cai, E. Breck, E. Nielsen, M. Salib, and D. Sculley, “Ten-\\nsorﬂow debugger: Debugging dataﬂow graphs for machine\\nlearning,” in Proceedings of the Reliable Machine Learning\\nin the Wild - NIPS 2016 Workshop , 2016.\\n[18] A. Gawande, Checklist Manifesto, The . Henry Holt and\\nCompany, 2009.\\n[19] D. Baylor, E. Breck, H.-T. Cheng, N. Fiedel, C. Y . Foo,\\nZ. Haque, S. Haykal, M. Ispir, V . Jain, L. Koc, C. Y . Koo,\\nL. Lew, C. Mewald, A. N. Modi, N. Polyzotis, S. Ramesh,\\nS. Roy, S. E. Whang, M. Wicke, J. Wilkiewicz, X. Zhang,\\nand M. Zinkevich, “Tfx: A tensorﬂow-based production-scale\\nmachine learning platform,” in KDD 2017 , 2017.\\n[20] M. T. Ribeiro, S. Singh, and C. Guestrin, “”why should\\nI trust you?”: Explaining the predictions of any classiﬁer,”\\nCoRR , vol. abs/1602.04938, 2016. [Online]. Available:\\nhttp://arxiv.org/abs/1602.04938', lookup_str='', metadata={'source': './docs/46555-3.pdf', 'page': 9}, lookup_index=0)]\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import OnlinePDFLoader, UnstructuredPDFLoader, PyMuPDFLoader,PyPDFLoader\n",
    "\n",
    "from langchain.vectorstores import FAISS, Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "from chromadb.utils import embedding_functions\n",
    "#works only with unstructuredpdfloader\n",
    "#pymupdf pdf is the fastest \n",
    "\n",
    "prompt_book = doc_dir + \"46555-3.pdf\"\n",
    "pdf_loader = PyPDFLoader(prompt_book)\n",
    "\n",
    "pdf = pdf_loader.load()\n",
    "print(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The ML Test Score:\\nA Rubric for ML Production Readiness and Technical Debt Reduction\\nEric Breck, Shanqing Cai, Eric Nielsen, Michael Salib, D. Sculley\\nGoogle, Inc.\\nebreck, cais, nielsene, msalib, dsculley @google.com\\nAbstract —Creating reliable, production-level machine learn-\\ning systems brings on a host of concerns not found in\\nsmall toy examples or even large ofﬂine research experiments.\\nTesting and monitoring are key considerations for ensuring\\nthe production-readiness of an ML system, and for reducing\\ntechnical debt of ML systems. But it can be difﬁcult to formu-\\nlate speciﬁc tests, given that the actual prediction behavior of\\nany given model is difﬁcult to specify a priori . In this paper,\\nwe present 28 speciﬁc tests and monitoring needs, drawn from\\nexperience with a wide range of production ML systems to help\\nquantify these issues and present an easy to follow road-map\\nto improve production readiness and pay down ML technical\\ndebt.\\nKeywords -Machine Learning, Testing, Monitoring, Reliabil-\\nity, Best Practices, Technical Debt\\nI. I NTRODUCTION\\nAs machine learning (ML) systems continue to take on\\never more central roles in real-world production settings,\\nthe issue of ML reliability has become increasingly critical.\\nML reliability involves a host of issues not found in small\\ntoy examples or even large ofﬂine experiments, which can\\nlead to surprisingly large amounts of technical debt [1].\\nTesting and monitoring are important strategies for improv-\\ning reliability, reducing technical debt, and lowering long-\\nterm maintenance cost. However, as suggested by Figure\\n1, ML system testing is also more complex a challenge\\nthan testing manually coded systems, due to the fact that\\nML system behavior depends strongly on data and models\\nthat cannot be strongly speciﬁed a priori . One way to see\\nthis is to consider ML training as analogous to compilation,\\nwhere the source is both code and training data. By that\\nanalogy, training data needs testing like code, and a trained\\nML model needs production practices like a binary does,\\nsuch as debuggability, rollbacks and monitoring.\\nSo, what should be tested and how much is enough?\\nIn this paper, we try to answer this question with a test\\nrubric , which is based on engineering decades of production-\\nlevel ML systems at Google, in systems such as ad click\\nprediction [2] and the Sibyl ML platform [3].\\nWe present a rubric as a set of 28 actionable tests, and\\noffer a scoring system to measure how ready for production\\na given machine learning system is. This rubric is intended\\nto cover a range from a team just starting out with machine\\nlearning up through tests that even a well-established teammay ﬁnd difﬁcult. Note that this rubric focuses on issues\\nspeciﬁc to ML systems, and so does not include generic\\nsoftware engineering best practices such as ensuring good\\nunit test coverage and a well-deﬁned binary release process.\\nSuch strategies remain necessary as well. We do call out\\na few speciﬁc areas for unit or integration tests that have\\nunique ML-related behavior.\\nHow to read the tests: Each test is written as an\\nassertion; our recommendation is to test that the assertion is\\ntrue, the more frequently the better, and to ﬁx the system if\\nthe assertion is not true.\\nDoesn’t this all go without saying?: Before we enu-\\nmerate our suggested tests, we should address one objection\\nthe reader may have – obviously one should write tests for\\nan engineering project! While this is true in principle, in a\\nsurvey of several dozen teams at Google, none of these tests\\nwas implemented by more than 80% of teams (though, even\\nin a engineering culture valuing rigorous testing, many of\\nthese ML-centric tests are non-obvious). Conversely, most\\ntests had a nonzero score for at least half of the teams\\nsurveyed; our tests do represent practices that teams ﬁnd\\nto be worth doing.\\nIn this paper, we are largely concerned with supervised\\nML systems that are trained continuously online and perform\\nrapid, low-latency inference on a server. Features are often\\nderived from large amounts of data such as streaming logs\\nof incoming data. However, most of our recommendations\\napply to other forms of ML systems, such as infrequently\\ntrained models pushed to client-side systems for inference.\\nA. Related work\\nSoftware testing is well studied, as is machine learning,\\nbut their intersection has been less well explored in the\\nliterature. [4] reviews testing for scientiﬁc software more\\ngenerally, and cites a number of articles such as [5], who\\npresent an approach for testing ML algorithms. These ideas\\nare a useful complement for the tests we present, which are\\nfocused on testing the use of ML in a production system\\nrather than just the correctness of the ML algorithm per se.\\nZinkevich provides extensive advice on building effective\\nmachine learning models in real world systems [6]. Those\\nrules are complementary to this rubric, which is more\\nc\\r2017 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media,\\nincluding reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to\\nservers or lists, or reuse of any copyrighted component of this work in other works. Published as [7].'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export OPENAI_API_KEY='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'"
     ]
    }
   ],
   "source": [
    "!cat access.zshrc\n",
    "!source access.zshrc\n",
    "!echo $OPENAI_API_KEY\n",
    "print(os.environ.get(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_content': ModelField(name='page_content', type=str, required=True),\n",
       " 'lookup_str': ModelField(name='lookup_str', type=str, required=False, default=''),\n",
       " 'metadata': ModelField(name='metadata', type=dict, required=False, default_factory='<function dict>'),\n",
       " 'lookup_index': ModelField(name='lookup_index', type=int, required=False, default=0)}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf[0].__fields__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "\n",
    "for i,_ in enumerate(pdf):\n",
    "    texts.append(pdf[i].page_content)\n",
    "\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Chroma using direct local API.\n",
      "Using DuckDB in-memory for database. Data will be transient.\n"
     ]
    }
   ],
   "source": [
    "OPENAI_API_KEY = \"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\"\n",
    "chroma_index = embedding_functions.OpenAIEmbeddingFunction(api_key= OPENAI_API_KEY,\n",
    "                                                           model_name= \"text-embedding-ada-002\")\n",
    "client = chromadb.Client(Settings(chroma_db_impl=\"duckdb\",\n",
    "                                  persist_directory= \"./embeds\"))\n",
    "\n",
    "embeds = chroma_index(texts= texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.get_or_create_collection(\"mlrubricscore\", embedding_function=chroma_index)\n",
    "collection.add(embeddings=embeds, documents=texts, ids = [str(i) for i in range(10)])\n",
    "#client.delete_collection(name=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['0', '7']],\n",
       " 'embeddings': None,\n",
       " 'documents': None,\n",
       " 'metadatas': None,\n",
       " 'distances': [[0.2766461670398712, 0.37192457914352417]]}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ques = \"What is the major rubric for readiness of a ML production pipeline\"\n",
    "collection.query(query_texts= ques, n_results=2, include= [\"distances\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for OpenAIEmbeddings\napi_key\n  extra fields not permitted (type=value_error.extra)\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[185], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m embs \u001b[39m=\u001b[39m OpenAIEmbeddings(api_key\u001b[39m=\u001b[39;49m OPENAI_API_KEY,\n\u001b[1;32m      2\u001b[0m                 model_name\u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mtext-embedding-ada-002\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/Desktop/Nlp notebooks/nlp_env/lib/python3.8/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 2 validation errors for OpenAIEmbeddings\napi_key\n  extra fields not permitted (type=value_error.extra)\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "embs = OpenAIEmbeddings(api_key= OPENAI_API_KEY,\n",
    "                model_name= \"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.query(query_embeddings= ques, n_results=2, include= [\"distances\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "print(collection.get(ids = [\"7\"])['documents'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = chroma_index.similarity_search(\"What is the major rubric for readiness of a ML production pipeline\", k = 1)\n",
    "for doc in docs:\n",
    "    print(str(doc.metadata[\"page\"]) + \":\", doc.page_content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Retrieval for journal and note taking"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
